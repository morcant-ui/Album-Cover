{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Levenshtein in /home/christine/miniconda3/lib/python3.12/site-packages (0.26.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/christine/miniconda3/lib/python3.12/site-packages (from Levenshtein) (3.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime\n",
    "from Levenshtein import distance\n",
    "from jinja2 import Template\n",
    "from PIL import Image \n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "from diffusers import AutoencoderKL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIOUS HELPER FUNCTIONS\n",
    "\n",
    "### SIMILARITY CALCULATIONS FOR TRACK NAMES\n",
    "\n",
    "SIMILARITY_DISTANCE = 3\n",
    "def are_tracks_similar(tracks):\n",
    "    for i in range(len(tracks)):\n",
    "        for j in range(i + 1, len(tracks)):\n",
    "            if distance(tracks[i]['name'], tracks[j]['name']) < SIMILARITY_DISTANCE:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def similarity_groups(tracks):\n",
    "    found_group = [False] * len(tracks)\n",
    "    groups = []\n",
    "\n",
    "    for i in range(len(tracks)):\n",
    "        if found_group[i]:\n",
    "            continue\n",
    "\n",
    "        found_group[i] = True\n",
    "        groups.append([tracks[i]])\n",
    "\n",
    "        for j in range(i + 1, len(tracks)):\n",
    "            if found_group[j]:\n",
    "                continue\n",
    "            if distance(tracks[i]['name'], tracks[j]['name']) < SIMILARITY_DISTANCE:\n",
    "                found_group[j] = True\n",
    "\n",
    "    return groups\n",
    "\n",
    "### OUTPUT FILE ID NAME\n",
    "def get_run_id(run_in):\n",
    "    model = run_in['model_id'].replace('/', '-')\n",
    "    return f\"{run_in['album_id']}_{run_in['positive-prompt']}_{run_in['negative-prompt']}_{run_in['inference_steps']}_{run_in['guidance_scale']}_{run_in['batch_size']}_{model}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET COMPUTER SPECS\n",
    "\n",
    "platform_info = {}\n",
    "\n",
    "platform_info['physical_cpu_cores'] = psutil.cpu_count(logical=False)\n",
    "platform_info['total_cpu_cores'] = psutil.cpu_count(logical=True)\n",
    "\n",
    "def get_available_device():\n",
    "    \"\"\"Helper method to find best possible hardware to run\n",
    "    Returns:\n",
    "        torch.device used to run experiments.\n",
    "        str representation of backend.\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"cuda\"\n",
    "\n",
    "    # Check if ROCm is available\n",
    "    if torch.version.hip is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"rocm\"), \"rocm\"\n",
    "\n",
    "    # Check if MPS (Apple Silicon) is available\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu'), \"mps\"\n",
    "\n",
    "    # Fall back to CPU\n",
    "    return torch.device(\"cpu\"), \"cpu\"\n",
    "\n",
    "# Check device info\n",
    "device, backend = get_available_device()\n",
    "\n",
    "# Check for GPU-specific details if CUDA or ROCm is available\n",
    "if device.type == \"cuda\":\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    cuda_version = torch.version.cuda\n",
    "elif device.type == \"rocm\":\n",
    "    cuda_device_count = torch.cuda.device_count()\n",
    "    cuda_device_name = torch.cuda.get_device_name(0)\n",
    "    cuda_version = torch.version.hip\n",
    "else:\n",
    "    cuda_device_count = 0\n",
    "    cuda_device_name = \"N/A\"\n",
    "    cuda_version = \"N/A\"\n",
    "\n",
    "platform_info['device'] = device.type\n",
    "platform_info['backend'] = backend\n",
    "platform_info['cuda_device_count'] = cuda_device_count\n",
    "platform_info['cuda_device_name'] = cuda_device_name\n",
    "platform_info['cuda_version'] = cuda_version\n",
    "\n",
    "# print(json.dumps(platform_info, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ALBUM DATA\n",
    "\n",
    "file_id = \"\" # if need for a specific album, put the file name here\n",
    "\n",
    "if file_id == \"\":\n",
    "    album_files = os.listdir('input/albums')\n",
    "    random_album_file = random.choice(album_files)\n",
    "else:\n",
    "    random_album_file = f'{file_id}.json'\n",
    "\n",
    "with open(f'input/albums/{random_album_file}', 'r') as file:\n",
    "    album_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSITIVE PROMPT TEMPLATES\n",
    "\n",
    "pos_prompt_templates = {}\n",
    "\n",
    "pos_prompt_templates['1-long'] =  Template(\"\"\"\\\n",
    "Album cover for this album:\n",
    "Album name : {{ album.name }}\n",
    "Artist{% if album.artists|length > 1 %}s{% endif %} : {{ album.artists | join(', ') }}\n",
    "Release Date : {{ album.date }}\n",
    "Label : {{ album.label }}\n",
    "Tracks:\n",
    "{% for track in album.tracks %}- {{ track.name }}\\n{% endfor %}\n",
    "\"\"\")\n",
    "\n",
    "pos_prompt_templates['2-only-tracks'] =  Template(\"\"\"\\\n",
    "Album cover for these tracks: \n",
    "{% for track in album.tracks %}- {{ track.name }}\\n{% endfor %}\n",
    "\"\"\")\n",
    "\n",
    "pos_prompt_templates['3-long-with-track-similarity'] =  Template(\"\"\"\\\n",
    "Album cover for this album:\n",
    "Album name : {{ album.name }}\n",
    "Artist{% if album.artists|length > 1 %}s{% endif %} : {{ album.artists | join(', ') }}\n",
    "Release Date : {{ album.date }}\n",
    "Label : {{ album.label }}\n",
    "\n",
    "{% if are_tracks_similar(album.tracks) %} Track format : {% for track in similarity_groups(album.tracks) %}- {{ track.name }}\\n{% endfor %}\n",
    "{% else %} Tracks:\n",
    "{% for track in album.tracks %}- {{ track.name }}\\n{% endfor %}{% endif %}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEGATIVE PROMPT TEMPLATES\n",
    "\n",
    "neg_prompts = {}\n",
    "neg_prompts['1-no-text'] = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER PARAMETERS\n",
    "\n",
    "INFERENCE_STEPS = [20, 100]\n",
    "GUIDANCE_SCALE = [5, 10]\n",
    "BATCH_SIZE = [1, 2]\n",
    "MODELS = ['sd-legacy/stable-diffusion-v1-5', 'stabilityai/stable-diffusion-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping prompt 3-long-with-track-similarity as tracks are not similar.\n",
      "Total runs: 32\n"
     ]
    }
   ],
   "source": [
    "# CREATE RUN PARAMETERS\n",
    "\n",
    "runs = []\n",
    "\n",
    "is_similar = are_tracks_similar(album_data['tracks'])\n",
    "\n",
    "for pos_key, template in pos_prompt_templates.items():\n",
    "    run_input = {\n",
    "        'computer_specs': platform_info,\n",
    "        'album_id': album_data['id'],\n",
    "    }\n",
    "\n",
    "    # Skip prompt if the tracks are not similar\n",
    "    if pos_key == '3-long-with-track-similarity' and not is_similar:\n",
    "        print(f\"Skipping prompt {pos_key} as tracks are not similar.\")\n",
    "        continue\n",
    "\n",
    "    run_input['positive-prompt'] = pos_key\n",
    "\n",
    "    for neg_key, neg_prompt in neg_prompts.items():\n",
    "        run_input['negative-prompt'] = neg_key\n",
    "\n",
    "        for step in INFERENCE_STEPS:\n",
    "            run_input['inference_steps'] = step\n",
    "\n",
    "            for scale in GUIDANCE_SCALE:\n",
    "                run_input['guidance_scale'] = scale\n",
    "\n",
    "                for batch in BATCH_SIZE:\n",
    "                    run_input['batch_size'] = batch\n",
    "\n",
    "                    for model in MODELS:\n",
    "                        run_input['model_id'] = model\n",
    "\n",
    "                        runs.append(run_input.copy())\n",
    "\n",
    "print(f\"Total runs: {len(runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #LOAD MODEL\n",
    "\n",
    "# # Stable Diffusion model: https://huggingface.co/stabilityai/stable-diffusion-2\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\", torch_dtype=torch.float32) \n",
    "# pipe = pipe.to(\"cuda\")  # Use \"cpu\" if CUDA is not available\n",
    "# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config) \n",
    "\n",
    "# #to improve quality \n",
    "# vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float32).to(\"cuda\")\n",
    "# pipe.vae = vae   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNS\n",
    "\n",
    "curr_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "output_dir = f'output/{curr_time}'\n",
    "os.makedirs(f'{output_dir}/images', exist_ok=True)\n",
    "os.makedirs(f'{output_dir}/runs', exist_ok=True)\n",
    "\n",
    "for run_input in runs:\n",
    "    # get ids for file naming\n",
    "    img_id = uuid.uuid4().hex[:8]\n",
    "    run_id = get_run_id(run_input)\n",
    "\n",
    "    positive_prompt = pos_prompt_templates[run_input['positive-prompt']].render(album=album_data, are_tracks_similar=are_tracks_similar, similarity_groups=similarity_groups)\n",
    "    negative_prompt = neg_prompts[run_input['negative-prompt']]\n",
    "\n",
    "    model_id = run_input['model_id']\n",
    "    inference_steps = run_input['inference_steps']\n",
    "    guidance_scale = run_input['guidance_scale']\n",
    "    batch_size = run_input['batch_size']\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO 1 : run the model here (remove temp images)\n",
    "    imgs = []\n",
    "    for i in range(batch_size):\n",
    "        # imgs.append(pipe(\n",
    "        #                 prompt= positive_prompt,\n",
    "        #                 negative_prompt= negative_prompt,\n",
    "        #                 guidance_scale= 1-5, #the higher the more it follows the prompt BUT lose in creativity\n",
    "        #                 num_inference_steps= 20, #the lower the faster but lose in quality \n",
    "        #                 #batch_size= 2,    \n",
    "        #             ).images[0])\n",
    "        imgs.append(Image.new('RGB', (256, 256), color = (73, 109, 137)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # TODO 2 : choose the measures to be returned\n",
    "    run_info = run_input\n",
    "    run_info['execution_time'] = end_time - start_time\n",
    "    run_info['resolution'] = 'N/A'\n",
    "    run_info['colour_quality'] = 'N/A'\n",
    "    run_info['ssim'] = 'N/A'\n",
    "    run_info['clip'] = 'N/A'\n",
    "    run_info['image_id'] = img_id\n",
    "\n",
    "    # Save the image(s)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img.save(f\"{output_dir}/images/{img_id}_{i}.png\")\n",
    "\n",
    "    # Save the run info\n",
    "    with open(f\"{output_dir}/runs/{run_id}.json\", 'w') as f:\n",
    "        json.dump(run_info, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
